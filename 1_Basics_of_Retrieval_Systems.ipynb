{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1IE26W9l3I6QQE-BQZsH3eJq8HSve9geF",
      "authorship_tag": "ABX9TyNQn5BP9GVb7HDxQ183dsLw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinakajoy/UsingLLMs-RAG-course/blob/main/1_Basics_of_Retrieval_Systems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval System"
      ],
      "metadata": {
        "id": "iAqe93HjUrSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "3eHZBz0W-Pb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample documents about LLMs, RAG and Retrieval System\n",
        "documents = [\n",
        " \"Large Language Models (LLMs) are AI systems trained on massive text corpora to generate and understand natural language.\",\n",
        " \"Retrieval-Augmented Generation (RAG) improves LLMs by allowing them to fetch external knowledge when answering questions.\",\n",
        " \"Vector databases like Pinecone, Weaviate, or FAISS are commonly used to power retrieval systems.\",\n",
        " \"LLMs often struggle with outdated or missing knowledge, which is why retrieval is essential for grounding responses.\",\n",
        " \"Context windows in LLMs limit how much information can be processed at once.\",\n",
        " \"Fine-tuning allows LLMs to specialize in a domain, but RAG can be a cheaper and more flexible alternative.\",\n",
        " \"Embeddings are numeric representations of text used to measure semantic similarity in retrieval systems.\",\n",
        " \"Hybrid search combines dense vector embeddings with keyword-based search for better accuracy.\",\n",
        " \"RAG systems often follow the retrieve-then-read pipeline: fetch documents and then use an LLM to generate an answer.\",\n",
        " \"Companies use retrieval systems to provide proprietary knowledge to LLMs without retraining the base model.\",\n",
        " \"Storing context in a vector database enables systems to handle large-scale document search efficiently.\",\n",
        " \"In-house LLM deployments are often combined with retrieval systems for privacy and security.\",\n",
        " \"Retrieval systems prevent hallucinations by grounding LLM outputs in verified sources.\",\n",
        " \"Scaling retrieval pipelines requires sharding, indexing, and efficient similarity search algorithms.\",\n",
        " \"LLMs like GPT, LLaMA, and Mistral can all be used in RAG setups.\",\n",
        " \"The retrieval step typically ranks documents based on semantic closeness to the user’s query.\",\n",
        " \"Chunking documents into smaller pieces improves retrieval accuracy and relevance.\",\n",
        " \"Metadata filters in vector search allow narrowing results by tags like date, author, or topic.\",\n",
        " \"Retrieval can be combined with caching to speed up repeated queries in production systems.\",\n",
        " \"Some RAG systems use graph databases instead of vectors for knowledge-rich retrieval.\",\n",
        " \"Evaluation of RAG involves metrics like precision, recall, and answer faithfulness.\",\n",
        " \"LLMs alone are generative, but when paired with retrieval, they become more reliable knowledge assistants.\",\n",
        " \"Context storage solutions range from simple in-memory stores to distributed vector databases.\",\n",
        " \"Retrieval systems can be domain-specific, such as for healthcare, legal, or financial data.\",\n",
        " \"LLMs with RAG are increasingly used in enterprise chatbots and knowledge assistants.\",\n",
        " \"Prompt engineering is often combined with retrieval to guide LLMs in how to use the fetched data.\",\n",
        " \"Scaling foundational models often requires techniques like parameter-efficient fine-tuning or LoRA.\",\n",
        " \"Retrieval helps reduce the need for frequent fine-tuning when new knowledge becomes available.\",\n",
        " \"Latency in retrieval systems is critical, especially when powering real-time applications.\",\n",
        " \"RAG pipelines can integrate with search engines, APIs, or internal document repositories.\",\n",
        " \"Knowledge grounding ensures that LLMs provide answers supported by external evidence.\",\n",
        " \"Some retrieval systems use re-ranking models to improve the quality of the top search results.\",\n",
        " \"Building a RAG system typically involves three steps: embedding, indexing, and retrieval.\",\n",
        " \"Retrieval allows organizations to keep proprietary knowledge private while still leveraging LLMs.\",\n",
        " \"An orchestration layer manages how LLMs, retrieval, and other tools interact in complex pipelines.\",\n",
        " \"LLMs combined with RAG are often described as knowledge-enhanced generative AI.\",\n",
        " \"The retriever can use dense embeddings or sparse methods like BM25, depending on the application.\",\n",
        " \"Chunk size in embeddings has a major effect on recall and precision in retrieval.\",\n",
        " \"Open-source libraries like LangChain and LlamaIndex simplify building retrieval-augmented systems.\",\n",
        " \"Document pre-processing, such as cleaning and normalization, is critical for good retrieval performance.\",\n",
        " \"Embedding models like OpenAI’s text-embedding-ada-002 or Sentence-BERT are popular for retrieval.\",\n",
        " \"RAG pipelines can handle both structured and unstructured data sources.\",\n",
        " \"Retrieval-based systems can log citations, giving users confidence in the generated answers.\",\n",
        " \"Distributed retrieval systems use multiple servers to scale to billions of documents.\",\n",
        " \"Context windows in LLMs can be extended with retrieval, allowing access to more knowledge.\",\n",
        " \"Retrieval can be applied in multi-modal systems, not just text but also images or audio.\",\n",
        " \"Enterprises prefer retrieval over fine-tuning when their data changes frequently.\",\n",
        " \"RAG systems can be evaluated with user satisfaction metrics in real-world applications.\",\n",
        " \"Building a good retrieval system requires balancing speed, accuracy, and storage costs.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "5SWWhjx4yn5R"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "2QlsZoB0y6bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "aS3hwibqy9Jp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download some models from nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vahbsb7Kzndn",
        "outputId": "2ab3108d-81db-4bd6-f8d0-81ac21b17de0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "sZgoyten0XCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample texts\n",
        "text1 = \"Retrieval-Augmented Generation (RAG) improves LLMs by allowing them to fetch external knowledge when answering questions.\"\n",
        "text2 = \"Retrieval-Augmented Generation (RAG) improves LLMs.It allows them to fetch external knowledge when answering questions.\"\n",
        "text3 = \"Retrieval-Augmented Generation (RAG) improves LLMs. It allows them to fetch external knowledge when answering questions.\""
      ],
      "metadata": {
        "id": "Ysk8VaE10W1x"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization into sentences\n",
        "print(nltk.sent_tokenize(text1))\n",
        "print(nltk.sent_tokenize(text2))\n",
        "print(nltk.sent_tokenize(text3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05XqesgE0SjC",
        "outputId": "6eecab2b-8fd4-4b90-cd67-619a16f6dbfd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Retrieval-Augmented Generation (RAG) improves LLMs by allowing them to fetch external knowledge when answering questions.']\n",
            "['Retrieval-Augmented Generation (RAG) improves LLMs.It allows them to fetch external knowledge when answering questions.']\n",
            "['Retrieval-Augmented Generation (RAG) improves LLMs.', 'It allows them to fetch external knowledge when answering questions.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A sentence is defined when there is one `.` followed by a space. More than one `.` won't break the sentence but be seen as one sentence"
      ],
      "metadata": {
        "id": "aEzkJy7r1Vzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization into words (a space defines a word)\n",
        "print(nltk.word_tokenize(text3))\n",
        "nltk.word_tokenize(text1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuPc79wx1AuC",
        "outputId": "64f1318d-96ce-450c-d56c-8c5c342a1c7c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Retrieval-Augmented', 'Generation', '(', 'RAG', ')', 'improves', 'LLMs', '.', 'It', 'allows', 'them', 'to', 'fetch', 'external', 'knowledge', 'when', 'answering', 'questions', '.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Retrieval-Augmented',\n",
              " 'Generation',\n",
              " '(',\n",
              " 'RAG',\n",
              " ')',\n",
              " 'improves',\n",
              " 'LLMs',\n",
              " 'by',\n",
              " 'allowing',\n",
              " 'them',\n",
              " 'to',\n",
              " 'fetch',\n",
              " 'external',\n",
              " 'knowledge',\n",
              " 'when',\n",
              " 'answering',\n",
              " 'questions',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "GUK2SFE92XHr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We do not capitalize. I would query 'improves llms' and not 'improves LLMs'\n",
        "* llms is different from LLMs\n",
        "* We don't really add punctuation"
      ],
      "metadata": {
        "id": "r8EK0Ei42rb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess function 1\n",
        "def preprocess(text):\n",
        "  # Convert to lowercase\n",
        "  text_lower = text.lower()\n",
        "\n",
        "  # Tokenize into words\n",
        "  tokens = nltk.word_tokenize(text_lower)\n",
        "\n",
        "  return [word for word in tokens if word.isalnum()]"
      ],
      "metadata": {
        "id": "OAUO2BPo1zu3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the pre-processing to the documents\n",
        "proprocessed_docs = [' '.join(preprocess(doc)) for doc in documents]"
      ],
      "metadata": {
        "id": "XtStfHTW2WA5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Different Retrieval Systems"
      ],
      "metadata": {
        "id": "XJqUanMw61Vk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Space Model (TF-IDF)"
      ],
      "metadata": {
        "id": "ItRStK5iNc3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an instance of the TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "QpRH-vo5NbvY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit and transform the preprocessed docs\n",
        "tfidf_matrix = vectorizer.fit_transform(proprocessed_docs)\n",
        "print(f\"The shape of the TF-IDF matrix is {tfidf_matrix.shape}\")\n",
        "print(f\"The length of the documents is {len(documents)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp1g4Si-OHc2",
        "outputId": "eea263f2-0aca-4357-d90c-9df3ec014dcb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of the TF-IDF matrix is (49, 291)\n",
            "The length of the documents is 49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the index\n",
        "query = \"improves llms\"\n",
        "query_vec = vectorizer.transform([query])\n",
        "cosine_similarity(tfidf_matrix, query_vec).flatten()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-orgc9RPAG1",
        "outputId": "df719b05-e3c9-497b-8bc6-0d0f9a7b4902"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.06420977, 0.33415949, 0.        , 0.07005281, 0.0811942 ,\n",
              "       0.08171789, 0.        , 0.        , 0.        , 0.08066559,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.08890166,\n",
              "       0.        , 0.28775125, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.07808653, 0.        , 0.        , 0.09660699,\n",
              "       0.07918326, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.08560015, 0.        , 0.        , 0.084361  , 0.07849505,\n",
              "       0.10346295, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.09217574,\n",
              "       0.        , 0.        , 0.        , 0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the documents by similarity to the query\n",
        "similarities = cosine_similarity(tfidf_matrix, query_vec).flatten()\n",
        "sorted_similarities = list(enumerate(similarities))\n",
        "sorted(sorted_similarities, key=lambda x: x[1], reverse=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaA-ix4tPpcJ",
        "outputId": "6dfa3e77-70dc-46ef-bf14-063c24b5d125"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, np.float64(0.33415949496673886)),\n",
              " (16, np.float64(0.2877512460781136)),\n",
              " (35, np.float64(0.10346294516015833)),\n",
              " (24, np.float64(0.09660699134048674)),\n",
              " (44, np.float64(0.09217573730116045)),\n",
              " (14, np.float64(0.08890165726082282)),\n",
              " (30, np.float64(0.0856001523360016)),\n",
              " (33, np.float64(0.08436100339135313)),\n",
              " (5, np.float64(0.08171788621281637)),\n",
              " (4, np.float64(0.08119419863943664)),\n",
              " (9, np.float64(0.08066559496228859)),\n",
              " (25, np.float64(0.07918326096886749)),\n",
              " (34, np.float64(0.07849504934184123)),\n",
              " (21, np.float64(0.07808653322188236)),\n",
              " (3, np.float64(0.07005281261282652)),\n",
              " (0, np.float64(0.06420977109907239)),\n",
              " (2, np.float64(0.0)),\n",
              " (6, np.float64(0.0)),\n",
              " (7, np.float64(0.0)),\n",
              " (8, np.float64(0.0)),\n",
              " (10, np.float64(0.0)),\n",
              " (11, np.float64(0.0)),\n",
              " (12, np.float64(0.0)),\n",
              " (13, np.float64(0.0)),\n",
              " (15, np.float64(0.0)),\n",
              " (17, np.float64(0.0)),\n",
              " (18, np.float64(0.0)),\n",
              " (19, np.float64(0.0)),\n",
              " (20, np.float64(0.0)),\n",
              " (22, np.float64(0.0)),\n",
              " (23, np.float64(0.0)),\n",
              " (26, np.float64(0.0)),\n",
              " (27, np.float64(0.0)),\n",
              " (28, np.float64(0.0)),\n",
              " (29, np.float64(0.0)),\n",
              " (31, np.float64(0.0)),\n",
              " (32, np.float64(0.0)),\n",
              " (36, np.float64(0.0)),\n",
              " (37, np.float64(0.0)),\n",
              " (38, np.float64(0.0)),\n",
              " (39, np.float64(0.0)),\n",
              " (40, np.float64(0.0)),\n",
              " (41, np.float64(0.0)),\n",
              " (42, np.float64(0.0)),\n",
              " (43, np.float64(0.0)),\n",
              " (45, np.float64(0.0)),\n",
              " (46, np.float64(0.0)),\n",
              " (47, np.float64(0.0)),\n",
              " (48, np.float64(0.0))]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a function to search with TF-IDF\n",
        "def search_tfidf(query, vectorizer, tfidf_matrix):\n",
        "  # Vectorize the query\n",
        "  query_vec = vectorizer.transform([query])\n",
        "\n",
        "  # Compute the Cosine Similarity\n",
        "  simlarities = cosine_similarity(tfidf_matrix, query_vec).flatten()\n",
        "\n",
        "  # Pair each document index with its similarity score\n",
        "  sorted_similarities =  list(enumerate(similarities))\n",
        "\n",
        "  # Sort the documents index with its similarity score\n",
        "  results = sorted(sorted_similarities, key=lambda x:x[1], reverse=True)\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "EFtpUB-GURZ_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function to the query\n",
        "search_similarities = search_tfidf(query, vectorizer, tfidf_matrix)\n",
        "\n",
        "# Print out the top 10 documents by similarity score\n",
        "print(f\"Top 10 documents by similarity score for query \\\"{query}\\\":\")\n",
        "for doc_index, score in search_similarities[:10]:\n",
        "  print(f\"Document {doc_index + 1}: {documents[doc_index]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_D0ysN5S499O",
        "outputId": "391ad0c3-2111-4dc5-b876-819c4abd9bee"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 documents by similarity score for query \"improves llms\":\n",
            "Document 2: Retrieval-Augmented Generation (RAG) improves LLMs by allowing them to fetch external knowledge when answering questions.\n",
            "Document 17: Chunking documents into smaller pieces improves retrieval accuracy and relevance.\n",
            "Document 36: LLMs combined with RAG are often described as knowledge-enhanced generative AI.\n",
            "Document 25: LLMs with RAG are increasingly used in enterprise chatbots and knowledge assistants.\n",
            "Document 45: Context windows in LLMs can be extended with retrieval, allowing access to more knowledge.\n",
            "Document 15: LLMs like GPT, LLaMA, and Mistral can all be used in RAG setups.\n",
            "Document 31: Knowledge grounding ensures that LLMs provide answers supported by external evidence.\n",
            "Document 34: Retrieval allows organizations to keep proprietary knowledge private while still leveraging LLMs.\n",
            "Document 6: Fine-tuning allows LLMs to specialize in a domain, but RAG can be a cheaper and more flexible alternative.\n",
            "Document 5: Context windows in LLMs limit how much information can be processed at once.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boolean Retrieval Model"
      ],
      "metadata": {
        "id": "GZKBsr8j6AUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install whoosh -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMxj5R4o5jB6",
        "outputId": "ef72a0b0-05d3-4735-aebf-b7a77e1e6c0c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/468.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/468.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m460.8/468.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import shutil\n",
        "from whoosh.index import create_in\n",
        "from whoosh.fields import *\n",
        "from whoosh.qparser import QueryParser"
      ],
      "metadata": {
        "id": "bjLKS2197zH7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess function 2\n",
        "def preprocess2(text):\n",
        "  # Convert to lowercase\n",
        "  text_lower = text.lower()\n",
        "\n",
        "  # Tokenize into words\n",
        "  tokens = nltk.word_tokenize(text_lower)\n",
        "\n",
        "  # List the tokens per document\n",
        "  tokens = [word for word in tokens if word.isalnum()]\n",
        "\n",
        "  # Define the English stopwords\n",
        "  stopwords = set(nltk.corpus.stopwords.words('english')) - {\"and\", \"or\", \"not\"}\n",
        "\n",
        "  # Remove the stopwords\n",
        "  tokens = [word for word in tokens if word not in stopwords]\n",
        "\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "RDWYuDfc8L6n"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply and test the function on text1 above\n",
        "print(text1)\n",
        "preprocess2(text1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql9fx7cq9ZwC",
        "outputId": "6e9e4651-bc2e-4503-b656-c7ae3e1230ce"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval-Augmented Generation (RAG) improves LLMs by allowing them to fetch external knowledge when answering questions.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['generation',\n",
              " 'rag',\n",
              " 'improves',\n",
              " 'llms',\n",
              " 'allowing',\n",
              " 'fetch',\n",
              " 'external',\n",
              " 'knowledge',\n",
              " 'answering',\n",
              " 'questions']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Basics of Retrieval System"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SSQPeKm9l0X",
        "outputId": "e7556a06-4942-4ff3-f151-2f4c401f96f3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Basics of Retrieval System\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new folder but remove old one first if available\n",
        "if os.path.exists(\"index_dir\"):\n",
        "  shutil.rmtree(\"index_dir\")\n",
        "os.mkdir(\"index_dir\")"
      ],
      "metadata": {
        "id": "S_6X98DG-IKi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Schema for the directory\n",
        "schema = Schema(title=ID(stored=True, unique=True),\n",
        "                content=TEXT(stored=True))"
      ],
      "metadata": {
        "id": "yiRCShZd-sxv"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the index in the folder\n",
        "index = create_in(\"index_dir\", schema)"
      ],
      "metadata": {
        "id": "wTNdkCIB_GMg"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open a writer to add documents to the index\n",
        "writer = index.writer()\n",
        "for i, doc in enumerate(documents):\n",
        "  writer.add_document(title=str(i),\n",
        "                      content=doc)\n",
        "writer.commit()"
      ],
      "metadata": {
        "id": "73DUAc-A_QzI"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boolean search function\n",
        "def boolean_search(query, index):\n",
        "  # Create a QueryParser that targets the content field\n",
        "  parser = QueryParser(\"content\", schema=index.schema)\n",
        "\n",
        "  # Parse the user's query\n",
        "  parsed_query = parser.parse(query)\n",
        "\n",
        "  # Open the directory and perform the query\n",
        "  with index.searcher() as searcher:\n",
        "    results = searcher.search(parsed_query)\n",
        "    return [(hit[\"title\"], hit[\"content\"]) for hit in results]"
      ],
      "metadata": {
        "id": "F5Lw6Xnu_1_x"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function\n",
        "query = \"LLMs not RAG\"\n",
        "boolean_search(query, index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-OKpUbv_rh_",
        "outputId": "3fb3767a-31d9-427f-c7eb-04442c95aad8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('24',\n",
              "  'LLMs with RAG are increasingly used in enterprise chatbots and knowledge assistants.'),\n",
              " ('14', 'LLMs like GPT, LLaMA, and Mistral can all be used in RAG setups.'),\n",
              " ('35',\n",
              "  'LLMs combined with RAG are often described as knowledge-enhanced generative AI.'),\n",
              " ('5',\n",
              "  'Fine-tuning allows LLMs to specialize in a domain, but RAG can be a cheaper and more flexible alternative.'),\n",
              " ('1',\n",
              "  'Retrieval-Augmented Generation (RAG) improves LLMs by allowing them to fetch external knowledge when answering questions.')]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Probabilistic Retrieval Model"
      ],
      "metadata": {
        "id": "6Ukqm8IZ7EI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25 -q"
      ],
      "metadata": {
        "id": "Sixrybh07JJ4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the BM25 class\n",
        "from rank_bm25 import BM25Okapi"
      ],
      "metadata": {
        "id": "Q3fRtIEiBVd3"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the documents => preprocess function is used because we don't need to remove stopwords since we are using BM25 which is probabilistic\n",
        "tokenized_docs = [preprocess(doc) for doc in documents]\n",
        "tokenized_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sm9llOc1Beua",
        "outputId": "74877018-ef3e-46ec-ac05-3ad4ceb778d8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['large',\n",
              "  'language',\n",
              "  'models',\n",
              "  'llms',\n",
              "  'are',\n",
              "  'ai',\n",
              "  'systems',\n",
              "  'trained',\n",
              "  'on',\n",
              "  'massive',\n",
              "  'text',\n",
              "  'corpora',\n",
              "  'to',\n",
              "  'generate',\n",
              "  'and',\n",
              "  'understand',\n",
              "  'natural',\n",
              "  'language'],\n",
              " ['generation',\n",
              "  'rag',\n",
              "  'improves',\n",
              "  'llms',\n",
              "  'by',\n",
              "  'allowing',\n",
              "  'them',\n",
              "  'to',\n",
              "  'fetch',\n",
              "  'external',\n",
              "  'knowledge',\n",
              "  'when',\n",
              "  'answering',\n",
              "  'questions'],\n",
              " ['vector',\n",
              "  'databases',\n",
              "  'like',\n",
              "  'pinecone',\n",
              "  'weaviate',\n",
              "  'or',\n",
              "  'faiss',\n",
              "  'are',\n",
              "  'commonly',\n",
              "  'used',\n",
              "  'to',\n",
              "  'power',\n",
              "  'retrieval',\n",
              "  'systems'],\n",
              " ['llms',\n",
              "  'often',\n",
              "  'struggle',\n",
              "  'with',\n",
              "  'outdated',\n",
              "  'or',\n",
              "  'missing',\n",
              "  'knowledge',\n",
              "  'which',\n",
              "  'is',\n",
              "  'why',\n",
              "  'retrieval',\n",
              "  'is',\n",
              "  'essential',\n",
              "  'for',\n",
              "  'grounding',\n",
              "  'responses'],\n",
              " ['context',\n",
              "  'windows',\n",
              "  'in',\n",
              "  'llms',\n",
              "  'limit',\n",
              "  'how',\n",
              "  'much',\n",
              "  'information',\n",
              "  'can',\n",
              "  'be',\n",
              "  'processed',\n",
              "  'at',\n",
              "  'once'],\n",
              " ['allows',\n",
              "  'llms',\n",
              "  'to',\n",
              "  'specialize',\n",
              "  'in',\n",
              "  'a',\n",
              "  'domain',\n",
              "  'but',\n",
              "  'rag',\n",
              "  'can',\n",
              "  'be',\n",
              "  'a',\n",
              "  'cheaper',\n",
              "  'and',\n",
              "  'more',\n",
              "  'flexible',\n",
              "  'alternative'],\n",
              " ['embeddings',\n",
              "  'are',\n",
              "  'numeric',\n",
              "  'representations',\n",
              "  'of',\n",
              "  'text',\n",
              "  'used',\n",
              "  'to',\n",
              "  'measure',\n",
              "  'semantic',\n",
              "  'similarity',\n",
              "  'in',\n",
              "  'retrieval',\n",
              "  'systems'],\n",
              " ['hybrid',\n",
              "  'search',\n",
              "  'combines',\n",
              "  'dense',\n",
              "  'vector',\n",
              "  'embeddings',\n",
              "  'with',\n",
              "  'search',\n",
              "  'for',\n",
              "  'better',\n",
              "  'accuracy'],\n",
              " ['rag',\n",
              "  'systems',\n",
              "  'often',\n",
              "  'follow',\n",
              "  'the',\n",
              "  'pipeline',\n",
              "  'fetch',\n",
              "  'documents',\n",
              "  'and',\n",
              "  'then',\n",
              "  'use',\n",
              "  'an',\n",
              "  'llm',\n",
              "  'to',\n",
              "  'generate',\n",
              "  'an',\n",
              "  'answer'],\n",
              " ['companies',\n",
              "  'use',\n",
              "  'retrieval',\n",
              "  'systems',\n",
              "  'to',\n",
              "  'provide',\n",
              "  'proprietary',\n",
              "  'knowledge',\n",
              "  'to',\n",
              "  'llms',\n",
              "  'without',\n",
              "  'retraining',\n",
              "  'the',\n",
              "  'base',\n",
              "  'model'],\n",
              " ['storing',\n",
              "  'context',\n",
              "  'in',\n",
              "  'a',\n",
              "  'vector',\n",
              "  'database',\n",
              "  'enables',\n",
              "  'systems',\n",
              "  'to',\n",
              "  'handle',\n",
              "  'document',\n",
              "  'search',\n",
              "  'efficiently'],\n",
              " ['llm',\n",
              "  'deployments',\n",
              "  'are',\n",
              "  'often',\n",
              "  'combined',\n",
              "  'with',\n",
              "  'retrieval',\n",
              "  'systems',\n",
              "  'for',\n",
              "  'privacy',\n",
              "  'and',\n",
              "  'security'],\n",
              " ['retrieval',\n",
              "  'systems',\n",
              "  'prevent',\n",
              "  'hallucinations',\n",
              "  'by',\n",
              "  'grounding',\n",
              "  'llm',\n",
              "  'outputs',\n",
              "  'in',\n",
              "  'verified',\n",
              "  'sources'],\n",
              " ['scaling',\n",
              "  'retrieval',\n",
              "  'pipelines',\n",
              "  'requires',\n",
              "  'sharding',\n",
              "  'indexing',\n",
              "  'and',\n",
              "  'efficient',\n",
              "  'similarity',\n",
              "  'search',\n",
              "  'algorithms'],\n",
              " ['llms',\n",
              "  'like',\n",
              "  'gpt',\n",
              "  'llama',\n",
              "  'and',\n",
              "  'mistral',\n",
              "  'can',\n",
              "  'all',\n",
              "  'be',\n",
              "  'used',\n",
              "  'in',\n",
              "  'rag',\n",
              "  'setups'],\n",
              " ['the',\n",
              "  'retrieval',\n",
              "  'step',\n",
              "  'typically',\n",
              "  'ranks',\n",
              "  'documents',\n",
              "  'based',\n",
              "  'on',\n",
              "  'semantic',\n",
              "  'closeness',\n",
              "  'to',\n",
              "  'the',\n",
              "  'user',\n",
              "  's',\n",
              "  'query'],\n",
              " ['chunking',\n",
              "  'documents',\n",
              "  'into',\n",
              "  'smaller',\n",
              "  'pieces',\n",
              "  'improves',\n",
              "  'retrieval',\n",
              "  'accuracy',\n",
              "  'and',\n",
              "  'relevance'],\n",
              " ['metadata',\n",
              "  'filters',\n",
              "  'in',\n",
              "  'vector',\n",
              "  'search',\n",
              "  'allow',\n",
              "  'narrowing',\n",
              "  'results',\n",
              "  'by',\n",
              "  'tags',\n",
              "  'like',\n",
              "  'date',\n",
              "  'author',\n",
              "  'or',\n",
              "  'topic'],\n",
              " ['retrieval',\n",
              "  'can',\n",
              "  'be',\n",
              "  'combined',\n",
              "  'with',\n",
              "  'caching',\n",
              "  'to',\n",
              "  'speed',\n",
              "  'up',\n",
              "  'repeated',\n",
              "  'queries',\n",
              "  'in',\n",
              "  'production',\n",
              "  'systems'],\n",
              " ['some',\n",
              "  'rag',\n",
              "  'systems',\n",
              "  'use',\n",
              "  'graph',\n",
              "  'databases',\n",
              "  'instead',\n",
              "  'of',\n",
              "  'vectors',\n",
              "  'for',\n",
              "  'retrieval'],\n",
              " ['evaluation',\n",
              "  'of',\n",
              "  'rag',\n",
              "  'involves',\n",
              "  'metrics',\n",
              "  'like',\n",
              "  'precision',\n",
              "  'recall',\n",
              "  'and',\n",
              "  'answer',\n",
              "  'faithfulness'],\n",
              " ['llms',\n",
              "  'alone',\n",
              "  'are',\n",
              "  'generative',\n",
              "  'but',\n",
              "  'when',\n",
              "  'paired',\n",
              "  'with',\n",
              "  'retrieval',\n",
              "  'they',\n",
              "  'become',\n",
              "  'more',\n",
              "  'reliable',\n",
              "  'knowledge',\n",
              "  'assistants'],\n",
              " ['context',\n",
              "  'storage',\n",
              "  'solutions',\n",
              "  'range',\n",
              "  'from',\n",
              "  'simple',\n",
              "  'stores',\n",
              "  'to',\n",
              "  'distributed',\n",
              "  'vector',\n",
              "  'databases'],\n",
              " ['retrieval',\n",
              "  'systems',\n",
              "  'can',\n",
              "  'be',\n",
              "  'such',\n",
              "  'as',\n",
              "  'for',\n",
              "  'healthcare',\n",
              "  'legal',\n",
              "  'or',\n",
              "  'financial',\n",
              "  'data'],\n",
              " ['llms',\n",
              "  'with',\n",
              "  'rag',\n",
              "  'are',\n",
              "  'increasingly',\n",
              "  'used',\n",
              "  'in',\n",
              "  'enterprise',\n",
              "  'chatbots',\n",
              "  'and',\n",
              "  'knowledge',\n",
              "  'assistants'],\n",
              " ['prompt',\n",
              "  'engineering',\n",
              "  'is',\n",
              "  'often',\n",
              "  'combined',\n",
              "  'with',\n",
              "  'retrieval',\n",
              "  'to',\n",
              "  'guide',\n",
              "  'llms',\n",
              "  'in',\n",
              "  'how',\n",
              "  'to',\n",
              "  'use',\n",
              "  'the',\n",
              "  'fetched',\n",
              "  'data'],\n",
              " ['scaling',\n",
              "  'foundational',\n",
              "  'models',\n",
              "  'often',\n",
              "  'requires',\n",
              "  'techniques',\n",
              "  'like',\n",
              "  'or',\n",
              "  'lora'],\n",
              " ['retrieval',\n",
              "  'helps',\n",
              "  'reduce',\n",
              "  'the',\n",
              "  'need',\n",
              "  'for',\n",
              "  'frequent',\n",
              "  'when',\n",
              "  'new',\n",
              "  'knowledge',\n",
              "  'becomes',\n",
              "  'available'],\n",
              " ['latency',\n",
              "  'in',\n",
              "  'retrieval',\n",
              "  'systems',\n",
              "  'is',\n",
              "  'critical',\n",
              "  'especially',\n",
              "  'when',\n",
              "  'powering',\n",
              "  'applications'],\n",
              " ['rag',\n",
              "  'pipelines',\n",
              "  'can',\n",
              "  'integrate',\n",
              "  'with',\n",
              "  'search',\n",
              "  'engines',\n",
              "  'apis',\n",
              "  'or',\n",
              "  'internal',\n",
              "  'document',\n",
              "  'repositories'],\n",
              " ['knowledge',\n",
              "  'grounding',\n",
              "  'ensures',\n",
              "  'that',\n",
              "  'llms',\n",
              "  'provide',\n",
              "  'answers',\n",
              "  'supported',\n",
              "  'by',\n",
              "  'external',\n",
              "  'evidence'],\n",
              " ['some',\n",
              "  'retrieval',\n",
              "  'systems',\n",
              "  'use',\n",
              "  'models',\n",
              "  'to',\n",
              "  'improve',\n",
              "  'the',\n",
              "  'quality',\n",
              "  'of',\n",
              "  'the',\n",
              "  'top',\n",
              "  'search',\n",
              "  'results'],\n",
              " ['building',\n",
              "  'a',\n",
              "  'rag',\n",
              "  'system',\n",
              "  'typically',\n",
              "  'involves',\n",
              "  'three',\n",
              "  'steps',\n",
              "  'embedding',\n",
              "  'indexing',\n",
              "  'and',\n",
              "  'retrieval'],\n",
              " ['retrieval',\n",
              "  'allows',\n",
              "  'organizations',\n",
              "  'to',\n",
              "  'keep',\n",
              "  'proprietary',\n",
              "  'knowledge',\n",
              "  'private',\n",
              "  'while',\n",
              "  'still',\n",
              "  'leveraging',\n",
              "  'llms'],\n",
              " ['an',\n",
              "  'orchestration',\n",
              "  'layer',\n",
              "  'manages',\n",
              "  'how',\n",
              "  'llms',\n",
              "  'retrieval',\n",
              "  'and',\n",
              "  'other',\n",
              "  'tools',\n",
              "  'interact',\n",
              "  'in',\n",
              "  'complex',\n",
              "  'pipelines'],\n",
              " ['llms',\n",
              "  'combined',\n",
              "  'with',\n",
              "  'rag',\n",
              "  'are',\n",
              "  'often',\n",
              "  'described',\n",
              "  'as',\n",
              "  'generative',\n",
              "  'ai'],\n",
              " ['the',\n",
              "  'retriever',\n",
              "  'can',\n",
              "  'use',\n",
              "  'dense',\n",
              "  'embeddings',\n",
              "  'or',\n",
              "  'sparse',\n",
              "  'methods',\n",
              "  'like',\n",
              "  'bm25',\n",
              "  'depending',\n",
              "  'on',\n",
              "  'the',\n",
              "  'application'],\n",
              " ['chunk',\n",
              "  'size',\n",
              "  'in',\n",
              "  'embeddings',\n",
              "  'has',\n",
              "  'a',\n",
              "  'major',\n",
              "  'effect',\n",
              "  'on',\n",
              "  'recall',\n",
              "  'and',\n",
              "  'precision',\n",
              "  'in',\n",
              "  'retrieval'],\n",
              " ['libraries',\n",
              "  'like',\n",
              "  'langchain',\n",
              "  'and',\n",
              "  'llamaindex',\n",
              "  'simplify',\n",
              "  'building',\n",
              "  'systems'],\n",
              " ['document',\n",
              "  'such',\n",
              "  'as',\n",
              "  'cleaning',\n",
              "  'and',\n",
              "  'normalization',\n",
              "  'is',\n",
              "  'critical',\n",
              "  'for',\n",
              "  'good',\n",
              "  'retrieval',\n",
              "  'performance'],\n",
              " ['embedding',\n",
              "  'models',\n",
              "  'like',\n",
              "  'openai',\n",
              "  's',\n",
              "  'or',\n",
              "  'are',\n",
              "  'popular',\n",
              "  'for',\n",
              "  'retrieval'],\n",
              " ['rag',\n",
              "  'pipelines',\n",
              "  'can',\n",
              "  'handle',\n",
              "  'both',\n",
              "  'structured',\n",
              "  'and',\n",
              "  'unstructured',\n",
              "  'data',\n",
              "  'sources'],\n",
              " ['systems',\n",
              "  'can',\n",
              "  'log',\n",
              "  'citations',\n",
              "  'giving',\n",
              "  'users',\n",
              "  'confidence',\n",
              "  'in',\n",
              "  'the',\n",
              "  'generated',\n",
              "  'answers'],\n",
              " ['distributed',\n",
              "  'retrieval',\n",
              "  'systems',\n",
              "  'use',\n",
              "  'multiple',\n",
              "  'servers',\n",
              "  'to',\n",
              "  'scale',\n",
              "  'to',\n",
              "  'billions',\n",
              "  'of',\n",
              "  'documents'],\n",
              " ['context',\n",
              "  'windows',\n",
              "  'in',\n",
              "  'llms',\n",
              "  'can',\n",
              "  'be',\n",
              "  'extended',\n",
              "  'with',\n",
              "  'retrieval',\n",
              "  'allowing',\n",
              "  'access',\n",
              "  'to',\n",
              "  'more',\n",
              "  'knowledge'],\n",
              " ['retrieval',\n",
              "  'can',\n",
              "  'be',\n",
              "  'applied',\n",
              "  'in',\n",
              "  'systems',\n",
              "  'not',\n",
              "  'just',\n",
              "  'text',\n",
              "  'but',\n",
              "  'also',\n",
              "  'images',\n",
              "  'or',\n",
              "  'audio'],\n",
              " ['enterprises',\n",
              "  'prefer',\n",
              "  'retrieval',\n",
              "  'over',\n",
              "  'when',\n",
              "  'their',\n",
              "  'data',\n",
              "  'changes',\n",
              "  'frequently'],\n",
              " ['rag',\n",
              "  'systems',\n",
              "  'can',\n",
              "  'be',\n",
              "  'evaluated',\n",
              "  'with',\n",
              "  'user',\n",
              "  'satisfaction',\n",
              "  'metrics',\n",
              "  'in',\n",
              "  'applications'],\n",
              " ['building',\n",
              "  'a',\n",
              "  'good',\n",
              "  'retrieval',\n",
              "  'system',\n",
              "  'requires',\n",
              "  'balancing',\n",
              "  'speed',\n",
              "  'accuracy',\n",
              "  'and',\n",
              "  'storage',\n",
              "  'costs']]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the BM25 model\n",
        "bm25 = BM25Okapi(tokenized_docs)"
      ],
      "metadata": {
        "id": "KI32DfoXCH7e"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# probabilistic search function\n",
        "def search_bm25(query, bm25):\n",
        "  tokenized_query = preprocess(query)\n",
        "  results = bm25.get_scores(tokenized_query)\n",
        "  return results"
      ],
      "metadata": {
        "id": "7YpOAjoaCWH8"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the probabilistic search\n",
        "query = \"improve llm\"\n",
        "\n",
        "# Perfom the BM25 search\n",
        "results = search_bm25(query, bm25)\n",
        "\n",
        "# Sort the documents by relevance to the query\n",
        "np.argsort(results)[::-1]\n",
        "\n",
        "# Print the douments\n",
        "for i in np.argsort(results)[::-1]:\n",
        "  print(f\"Document {i + 1}: {documents[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhZXXc2UDHl-",
        "outputId": "4fa7fb53-4d07-41e3-85a1-340e9f881f56"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 32: Some retrieval systems use re-ranking models to improve the quality of the top search results.\n",
            "Document 13: Retrieval systems prevent hallucinations by grounding LLM outputs in verified sources.\n",
            "Document 12: In-house LLM deployments are often combined with retrieval systems for privacy and security.\n",
            "Document 9: RAG systems often follow the retrieve-then-read pipeline: fetch documents and then use an LLM to generate an answer.\n",
            "Document 45: Context windows in LLMs can be extended with retrieval, allowing access to more knowledge.\n",
            "Document 48: RAG systems can be evaluated with user satisfaction metrics in real-world applications.\n",
            "Document 47: Enterprises prefer retrieval over fine-tuning when their data changes frequently.\n",
            "Document 46: Retrieval can be applied in multi-modal systems, not just text but also images or audio.\n",
            "Document 41: Embedding models like OpenAI’s text-embedding-ada-002 or Sentence-BERT are popular for retrieval.\n",
            "Document 40: Document pre-processing, such as cleaning and normalization, is critical for good retrieval performance.\n",
            "Document 39: Open-source libraries like LangChain and LlamaIndex simplify building retrieval-augmented systems.\n",
            "Document 38: Chunk size in embeddings has a major effect on recall and precision in retrieval.\n",
            "Document 37: The retriever can use dense embeddings or sparse methods like BM25, depending on the application.\n",
            "Document 44: Distributed retrieval systems use multiple servers to scale to billions of documents.\n",
            "Document 43: Retrieval-based systems can log citations, giving users confidence in the generated answers.\n",
            "Document 42: RAG pipelines can handle both structured and unstructured data sources.\n",
            "Document 49: Building a good retrieval system requires balancing speed, accuracy, and storage costs.\n",
            "Document 33: Building a RAG system typically involves three steps: embedding, indexing, and retrieval.\n",
            "Document 34: Retrieval allows organizations to keep proprietary knowledge private while still leveraging LLMs.\n",
            "Document 35: An orchestration layer manages how LLMs, retrieval, and other tools interact in complex pipelines.\n",
            "Document 36: LLMs combined with RAG are often described as knowledge-enhanced generative AI.\n",
            "Document 28: Retrieval helps reduce the need for frequent fine-tuning when new knowledge becomes available.\n",
            "Document 31: Knowledge grounding ensures that LLMs provide answers supported by external evidence.\n",
            "Document 30: RAG pipelines can integrate with search engines, APIs, or internal document repositories.\n",
            "Document 29: Latency in retrieval systems is critical, especially when powering real-time applications.\n",
            "Document 24: Retrieval systems can be domain-specific, such as for healthcare, legal, or financial data.\n",
            "Document 23: Context storage solutions range from simple in-memory stores to distributed vector databases.\n",
            "Document 22: LLMs alone are generative, but when paired with retrieval, they become more reliable knowledge assistants.\n",
            "Document 21: Evaluation of RAG involves metrics like precision, recall, and answer faithfulness.\n",
            "Document 20: Some RAG systems use graph databases instead of vectors for knowledge-rich retrieval.\n",
            "Document 27: Scaling foundational models often requires techniques like parameter-efficient fine-tuning or LoRA.\n",
            "Document 26: Prompt engineering is often combined with retrieval to guide LLMs in how to use the fetched data.\n",
            "Document 25: LLMs with RAG are increasingly used in enterprise chatbots and knowledge assistants.\n",
            "Document 17: Chunking documents into smaller pieces improves retrieval accuracy and relevance.\n",
            "Document 18: Metadata filters in vector search allow narrowing results by tags like date, author, or topic.\n",
            "Document 19: Retrieval can be combined with caching to speed up repeated queries in production systems.\n",
            "Document 16: The retrieval step typically ranks documents based on semantic closeness to the user’s query.\n",
            "Document 14: Scaling retrieval pipelines requires sharding, indexing, and efficient similarity search algorithms.\n",
            "Document 15: LLMs like GPT, LLaMA, and Mistral can all be used in RAG setups.\n",
            "Document 11: Storing context in a vector database enables systems to handle large-scale document search efficiently.\n",
            "Document 10: Companies use retrieval systems to provide proprietary knowledge to LLMs without retraining the base model.\n",
            "Document 8: Hybrid search combines dense vector embeddings with keyword-based search for better accuracy.\n",
            "Document 7: Embeddings are numeric representations of text used to measure semantic similarity in retrieval systems.\n",
            "Document 6: Fine-tuning allows LLMs to specialize in a domain, but RAG can be a cheaper and more flexible alternative.\n",
            "Document 5: Context windows in LLMs limit how much information can be processed at once.\n",
            "Document 4: LLMs often struggle with outdated or missing knowledge, which is why retrieval is essential for grounding responses.\n",
            "Document 3: Vector databases like Pinecone, Weaviate, or FAISS are commonly used to power retrieval systems.\n",
            "Document 2: Retrieval-Augmented Generation (RAG) improves LLMs by allowing them to fetch external knowledge when answering questions.\n",
            "Document 1: Large Language Models (LLMs) are AI systems trained on massive text corpora to generate and understand natural language.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sBd61lfFDrJF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}