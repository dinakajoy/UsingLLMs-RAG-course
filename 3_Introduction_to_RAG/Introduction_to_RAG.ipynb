{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE68DgxneTAa"
      },
      "source": [
        "# Retrieval Augumented Generation (RAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmpKNOWXecHO"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuLwfPEbeaI2"
      },
      "outputs": [],
      "source": [
        "# Sample documents about LLMs, RAG and Retrieval System\n",
        "documents = [\n",
        " \"Large Language Models (LLMs) are AI systems trained on massive text corpora to generate and understand natural language.\",\n",
        " \"Retrieval-Augmented Generation (RAG) improves LLMs by allowing them to fetch external knowledge when answering questions.\",\n",
        " \"Vector databases like Pinecone, Weaviate, or FAISS are commonly used to power retrieval systems.\",\n",
        " \"LLMs often struggle with outdated or missing knowledge, which is why retrieval is essential for grounding responses.\",\n",
        " \"Context windows in LLMs limit how much information can be processed at once.\",\n",
        " \"Fine-tuning allows LLMs to specialize in a domain, but RAG can be a cheaper and more flexible alternative.\",\n",
        " \"Embeddings are numeric representations of text used to measure semantic similarity in retrieval systems.\",\n",
        " \"Hybrid search combines dense vector embeddings with keyword-based search for better accuracy.\",\n",
        " \"RAG systems often follow the retrieve-then-read pipeline: fetch documents and then use an LLM to generate an answer.\",\n",
        " \"Companies use retrieval systems to provide proprietary knowledge to LLMs without retraining the base model.\",\n",
        " \"Storing context in a vector database enables systems to handle large-scale document search efficiently.\",\n",
        " \"In-house LLM deployments are often combined with retrieval systems for privacy and security.\",\n",
        " \"Retrieval systems prevent hallucinations by grounding LLM outputs in verified sources.\",\n",
        " \"Scaling retrieval pipelines requires sharding, indexing, and efficient similarity search algorithms.\",\n",
        " \"LLMs like GPT, LLaMA, and Mistral can all be used in RAG setups.\",\n",
        " \"The retrieval step typically ranks documents based on semantic closeness to the user’s query.\",\n",
        " \"Chunking documents into smaller pieces improves retrieval accuracy and relevance.\",\n",
        " \"Metadata filters in vector search allow narrowing results by tags like date, author, or topic.\",\n",
        " \"Retrieval can be combined with caching to speed up repeated queries in production systems.\",\n",
        " \"Some RAG systems use graph databases instead of vectors for knowledge-rich retrieval.\",\n",
        " \"Evaluation of RAG involves metrics like precision, recall, and answer faithfulness.\",\n",
        " \"LLMs alone are generative, but when paired with retrieval, they become more reliable knowledge assistants.\",\n",
        " \"Context storage solutions range from simple in-memory stores to distributed vector databases.\",\n",
        " \"Retrieval systems can be domain-specific, such as for healthcare, legal, or financial data.\",\n",
        " \"LLMs with RAG are increasingly used in enterprise chatbots and knowledge assistants.\",\n",
        " \"Prompt engineering is often combined with retrieval to guide LLMs in how to use the fetched data.\",\n",
        " \"Scaling foundational models often requires techniques like parameter-efficient fine-tuning or LoRA.\",\n",
        " \"Retrieval helps reduce the need for frequent fine-tuning when new knowledge becomes available.\",\n",
        " \"Latency in retrieval systems is critical, especially when powering real-time applications.\",\n",
        " \"RAG pipelines can integrate with search engines, APIs, or internal document repositories.\",\n",
        " \"Knowledge grounding ensures that LLMs provide answers supported by external evidence.\",\n",
        " \"Some retrieval systems use re-ranking models to improve the quality of the top search results.\",\n",
        " \"Building a RAG system typically involves three steps: embedding, indexing, and retrieval.\",\n",
        " \"Retrieval allows organizations to keep proprietary knowledge private while still leveraging LLMs.\",\n",
        " \"An orchestration layer manages how LLMs, retrieval, and other tools interact in complex pipelines.\",\n",
        " \"LLMs combined with RAG are often described as knowledge-enhanced generative AI.\",\n",
        " \"The retriever can use dense embeddings or sparse methods like BM25, depending on the application.\",\n",
        " \"Chunk size in embeddings has a major effect on recall and precision in retrieval.\",\n",
        " \"Open-source libraries like LangChain and LlamaIndex simplify building retrieval-augmented systems.\",\n",
        " \"Document pre-processing, such as cleaning and normalization, is critical for good retrieval performance.\",\n",
        " \"Embedding models like OpenAI’s text-embedding-ada-002 or Sentence-BERT are popular for retrieval.\",\n",
        " \"RAG pipelines can handle both structured and unstructured data sources.\",\n",
        " \"Retrieval-based systems can log citations, giving users confidence in the generated answers.\",\n",
        " \"Distributed retrieval systems use multiple servers to scale to billions of documents.\",\n",
        " \"Context windows in LLMs can be extended with retrieval, allowing access to more knowledge.\",\n",
        " \"Retrieval can be applied in multi-modal systems, not just text but also images or audio.\",\n",
        " \"Enterprises prefer retrieval over fine-tuning when their data changes frequently.\",\n",
        " \"RAG systems can be evaluated with user satisfaction metrics in real-world applications.\",\n",
        " \"Building a good retrieval system requires balancing speed, accuracy, and storage costs.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D0jlBLVekrp"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GvIviS6LehQJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "hf_key = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2kUdgryvj58D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = hf_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIEF9gztj-UD",
        "outputId": "1126bb92-f7cd-43fb-ae01-ca641acd8d9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain-huggingface sentence-transformers langchain_community faiss-cpu -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S4a5kSYekAIP"
      },
      "outputs": [],
      "source": [
        "# Import the libraries\n",
        "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain.docstore.document import Document\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt-a5LSBk1ko"
      },
      "source": [
        "## Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNigz6XSkz2U",
        "outputId": "d209de6a-ace4-4c7c-c1d8-55c22a3ee814"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We have 49 documents\n"
          ]
        }
      ],
      "source": [
        "# Check how many documents we have\n",
        "print(f\"We have {len(documents)} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VBYzC_EYlAax"
      },
      "outputs": [],
      "source": [
        "# Wrap each string in a Document\n",
        "docs = [Document(page_content=text) for text in documents]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "64e406d4f85f4421815b8d8661e73e5c",
            "da481f7e1d184a9b9d3b31dac567aba7",
            "d40c963746f54c4abaecb9c291c851a7",
            "9d95fcdf83df44d4a4f8cd7a0744d8c2",
            "7db8702778ed417ab6a63d1a066da114",
            "3feb07ebd42d49b9a17d2f737bb6028b",
            "a75f107396f44e9eae3a1d1f6ee9a902",
            "1ad79237656a44e2b59ffe34ccdb507a",
            "b59d67784a974b3c88496aecc3eb8110",
            "c31f0837021340b3b17bf281fceec9c2",
            "9242bd89d33345d4976ad9e7ba2fd2ee",
            "53fa9de4e30d4a3bb74779e687170698",
            "57fe8e6c603441f68ac7b21b64fe1a5f",
            "ce90c65770994888be7c809b6ef0d986",
            "753f86a7f3ae45d58c1e62458303b985",
            "24df9d9d6f4e4e63a1a842aaa8fb500d",
            "53a90b88bf184764900c69aa21caccac",
            "84cfcede9ae04397b71cd30a28832c07",
            "e73607081b2e45f9a05eabeddac85a74",
            "f6866ea3b4f942378691644c6d8c6521",
            "a8aaa863ef4e49bc8aaede5aa169c601",
            "b3453e5978274227a8db6fc532b14cd6",
            "9d5551d45cdf478191705b031dca6953",
            "7e0f61e5d4e345968dbd0e3fb9af06fa",
            "5e3eb1fee39f48679a9d64b3480b9ea3",
            "a6defe1fb8934e39b982eef4b117d2d9",
            "3bb4ea1b066a473888f6b4f7fd9ca8db",
            "b0d44482ed06456290dea23638c9d6cd",
            "b6c30d3d42174709bb7c85622cdbc03e",
            "42a7870d7b2e41ca85e81441d5bfcfdc",
            "552cccb1d5e24b1489c8179d0e3871e5",
            "a5920e5e62d945b4b0b4f3059f116b6e",
            "324312f44bb64d3b9ba7fb0262f20f5a",
            "a1c80598f7e142c4ae7d6e8d59dd66d7",
            "461af40933084ed49ac08fdb76926ca6",
            "8ef964bbf73f4aee84030f89ce14a6ab",
            "bc9b7cab33784d00aec380cc29fabe26",
            "fa7dbec23e9e487bb659550f93a2bbd1",
            "b1184f6415b74b21a8e797fd98aeb485",
            "e2783ba6c0e94e1387d5b0d1395ad28d",
            "46c034c603be4db9b6f98f1490f4c37c",
            "38dd90979290469b8c906fc1a84b8fc0",
            "f0d844524e2b4051aee4c977deb177a8",
            "358e7d84f59f49e08e2502f0826fa638",
            "59a7e7ab59fd46c0aa761629ac755226",
            "86140920acef4cb29f34b527095f855d",
            "df058ebabc8b499788c3781a11a9c77f",
            "c39b6d10d48e41bf8e5d48ad99ed6be8",
            "8d5887db2aca4e7ca8d678695ccf788c",
            "b501710da1dd46c58f5f7849fb8b16a3",
            "ef90c3cbbfe24a23ae02bc72ca131a8a",
            "071d058a2eb74028947e704b04b69d0c",
            "5f0be935d88246ccb89a1e0e6d55ca7d",
            "b0fb6461256b425194cdc42dfb28d878",
            "4f85b919fde94f8abc4933e36790397e",
            "ff38c875af63469287e488f2e00ca3c6",
            "df410fdbd82549ea931110ea0c82ab30",
            "c323cb16ae224af2a674acf8b90a3d0a",
            "2beee77cd91d4360913916264d81df76",
            "7f46a948beb344bda5f17fb04e35f2f3",
            "b139e7ca2b5d4abdac1ddc98a32343d4",
            "2304f039c0094b45b7ffaffb258b17c3",
            "16cc96e13fef4686a3d6819a65318c08",
            "0b138e0320214e0285c0dd3f1f30ae27",
            "e64bbc8533914ff782b721ff2516ce4a",
            "9fca20aa3e1144d9ad7b7d99255d95cd",
            "0b3b4974b0674ada81cd4363775e55b6",
            "efd3661355fe49238a84649e57c93269",
            "e2923bb2d8674885978a8ad98e085206",
            "9e77250876e142feb442ef53d234c072",
            "b1e72d6ce9da43b480cbd37218f2cb6a",
            "cbf495d47de846b1b98a223d842c7166",
            "a16ef4e329c64bc09556d9414c4297f1",
            "4273447c791d49d8b95944a0612a61dc",
            "dba5a7f582cc443394aa62c0ed8e3e77",
            "4179986cb52c45b1a9a72d07283df8f6",
            "d906cc0b493f472b8ab7b85135efb974",
            "146a663ff1cb4878bc5ca647aafc4fc2",
            "04f1c2b6092f4b33bf264b5321af6400",
            "cc69961fb5f24f69b7f4f221e4810b50",
            "5ed15c30bc3b4999a9c4b4da72c1eb3b",
            "cd25f15871254fa68f7cd106b7cc2e07",
            "b3bf696c8e064afd8fa90ed3821c15ac",
            "fe763101b6414c41a8282337c529cecb",
            "694e26a3bacd41bcab769d08444d4824",
            "d30da9f91bac4fbe82d0584c392fb0c0",
            "0150098a1b9442548070e639f99108e1",
            "4f72616c6d56459090e7d6363fdd0609",
            "46814ff5a44b4488872301bff3f7a927",
            "be78962c51874813b4ba3f2601e1cefe",
            "822c1327ff474a46a64f8ca9d99febaf",
            "0a01752bbee340ac857f530e8ee544ff",
            "da3653c2d1ad445295e05a3e61188183",
            "b9259a85cba0413db7116ba3df1b9983",
            "da58f7de787140c4b9a05e83f58bcff6",
            "d2fd0b71831549368c8b62f5061ff11d",
            "0d47b2a2f7a44ae3aef215a25c7c7fc9",
            "5780627956be496d9f65bec3bc0d1ab3",
            "993c596aa96d46c58e2a61b65b8cbafb",
            "64ba1fd57cc241b49642570acc2168dd",
            "4d129117281649dc823629261922d6af",
            "f0d04834f3144389a4b06096bc4139af",
            "01faf605e7a0408aaf8b74078c82fe0c",
            "c116450f89cb4a33ac87f02f8e521220",
            "97324367cf4e4b9a95d7736859e6c027",
            "f391fc2dfa4f4cfcb0b62468f2b361ba",
            "f15625d29ff249ddb947bc916c2831e2",
            "a6d1925c57314073ad4f054919477f8a",
            "eba735c7cc964fc29808ad50853cc1c6",
            "2ea4cf54648640daa647d41468069013",
            "9e2dfd9126c2480fafeb19f99f4af973",
            "8d52fd20d11940748fec0592373880e9",
            "6d8e89ec5d544becb4456079b3e1295d",
            "423a2c80ea594b02b7161e3655b8d9ec",
            "1784dd1f87bc4a2ca4ae0d38df6d0ab3",
            "d42713ed18574367b73be58947f78baa",
            "4b1dba9c1f3248f9996b6cef82aaee2f",
            "e1c9c8eec4f848c1a6a5a8ac2d77d4f6",
            "8def264077ec41788f736edc5801f905",
            "9c62eb23debb4ad48d5145f207a4c523",
            "586143494e0a47e187c71c8405197e87"
          ]
        },
        "id": "l5zEK-lblUoK",
        "outputId": "68cc013f-52f4-4442-d4e0-7dc4a663ec30"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "64e406d4f85f4421815b8d8661e73e5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53fa9de4e30d4a3bb74779e687170698",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d5551d45cdf478191705b031dca6953",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1c80598f7e142c4ae7d6e8d59dd66d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59a7e7ab59fd46c0aa761629ac755226",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff38c875af63469287e488f2e00ca3c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b3b4974b0674ada81cd4363775e55b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "146a663ff1cb4878bc5ca647aafc4fc2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46814ff5a44b4488872301bff3f7a927",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "64ba1fd57cc241b49642570acc2168dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e2dfd9126c2480fafeb19f99f4af973",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Use a transformer-based embedding model\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xJdwx5WJlgxj"
      },
      "outputs": [],
      "source": [
        "# Create a FAISS vector store for the docs\n",
        "faiss_store = FAISS.from_documents(docs, embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9cB3_qxlx0T",
        "outputId": "b4d1021b-3005-471f-a1b6-252fd3d11b33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of indexes: 49\n",
            "Total number of dimensions: 384\n",
            "First vector: [ 5.59799224e-02 -1.23615697e-01  5.04221395e-02 -2.13914042e-04\n",
            "  7.37531185e-02  8.95548984e-03 -6.00405224e-02  1.49373375e-02\n",
            "  5.68501055e-02  3.37935537e-02 -2.43983790e-02  7.96056166e-03\n",
            "  6.15357235e-02 -8.11576284e-03  1.36438720e-02  5.61926477e-02\n",
            "  7.67754018e-02  3.06968205e-03 -2.04155762e-02 -9.15290713e-02\n",
            "  7.54889697e-02  9.49257165e-02 -2.44868174e-02  7.48942839e-03\n",
            "  2.30758637e-02  3.10892090e-02 -7.53808301e-03 -1.41408620e-02\n",
            "  4.08316962e-02 -4.55593131e-02  1.01234503e-02  5.85593507e-02\n",
            "  7.60235637e-02  8.76820683e-02 -2.13428158e-02  6.30844012e-02\n",
            " -6.90986440e-02  4.78140311e-03  2.68640146e-02  6.28683064e-03\n",
            " -2.52215122e-03 -6.12178817e-03  7.45765865e-02 -6.18263520e-02\n",
            "  1.61846399e-01  4.47338596e-02 -8.18227157e-02  1.99009180e-02\n",
            " -4.68176790e-02  2.07310989e-02 -1.00719728e-01 -5.11116572e-02\n",
            " -1.06674517e-02  3.76786180e-02 -3.49273942e-02 -7.82647058e-02\n",
            "  3.20381373e-02 -1.67206535e-03 -3.04974969e-02 -4.60983142e-02\n",
            " -2.90375128e-02 -1.11909561e-01 -3.08425631e-02  2.96229981e-02\n",
            "  4.88636158e-02  2.27161609e-02 -1.27594126e-03  9.00783837e-02\n",
            " -2.33559106e-02 -5.88586181e-02  2.04303097e-02 -1.91230632e-04\n",
            " -2.00297628e-02  9.18395296e-02 -2.62243543e-02 -1.59554258e-02\n",
            "  2.46915072e-02  5.87892253e-03  1.11701906e-01 -2.40288768e-02\n",
            " -2.62684207e-02  6.26533329e-02  8.76337662e-02  1.01037351e-02\n",
            "  2.51335502e-02  2.29502912e-03  8.02783854e-03  4.09501828e-02\n",
            "  3.01650800e-02  1.23733953e-02 -9.87364165e-03 -1.33968309e-01\n",
            " -1.48515413e-02  2.84969490e-02 -1.95497740e-02  3.95998135e-02\n",
            " -2.23897658e-02 -4.30137143e-02  5.59214354e-02  4.12259735e-02\n",
            "  2.94184778e-02  7.40719140e-02  5.86774610e-02 -4.84780110e-02\n",
            " -6.83995113e-02 -2.56933831e-02  4.58729863e-02  2.31131911e-02\n",
            "  7.23061934e-02 -8.96524489e-02  8.51430057e-04  4.17844579e-02\n",
            "  1.86159015e-02  3.15907858e-02  7.67726600e-02 -1.13299012e-01\n",
            "  9.04402807e-02 -3.27261426e-02  1.91022847e-02  4.26131785e-02\n",
            " -9.80662405e-02  2.36903522e-02  1.28346484e-03  1.46677978e-02\n",
            "  1.14158010e-02  2.93858796e-02 -5.92334569e-02 -1.35153455e-33\n",
            "  6.24970458e-02  4.11282554e-02  9.54749435e-03  7.97725841e-02\n",
            "  6.24811426e-02 -1.60833308e-03  6.43271282e-02  7.05759376e-02\n",
            " -4.89395298e-02  7.55818840e-03 -8.24661851e-02  1.46082575e-02\n",
            " -6.78726565e-03  2.84265727e-02  3.04525718e-02 -2.73350812e-02\n",
            "  3.22145788e-04 -2.51294905e-03  5.46441227e-03 -4.16599847e-02\n",
            "  1.62200034e-02  2.48758085e-02  5.12611046e-02 -6.98485672e-02\n",
            "  2.62655709e-02  4.06459160e-02  8.73463750e-02 -5.45662232e-02\n",
            "  2.54323445e-02  1.68162603e-02 -7.06314445e-02  1.07546449e-02\n",
            " -1.46114072e-02  1.00386357e-02  2.65516695e-02  1.60414248e-03\n",
            " -7.74145573e-02 -8.63365177e-03  4.12104949e-02 -6.52849525e-02\n",
            "  4.83007589e-03  3.51354145e-02  3.35920639e-02 -3.78179103e-02\n",
            " -7.55131394e-02  4.55202870e-02 -1.80512276e-02 -4.48966287e-02\n",
            "  1.89132628e-03 -5.54836839e-02  8.75203237e-02 -9.03356820e-03\n",
            " -7.12265894e-02 -1.28765805e-02  5.74364401e-02  6.26431331e-02\n",
            " -5.76011203e-02  1.13319550e-02 -4.01937962e-02  1.42380744e-02\n",
            "  2.04041079e-02  2.93308180e-02  3.38891596e-02  7.32148066e-02\n",
            "  1.33543387e-01  2.68958621e-02 -5.48977405e-02  3.05433571e-02\n",
            "  8.66482034e-02 -3.48206162e-02  2.96646748e-02 -3.14270183e-02\n",
            " -1.71329379e-02 -1.05607854e-02 -3.67758982e-02 -7.99657255e-02\n",
            "  9.06064268e-03 -6.87769204e-02  1.05948746e-02  6.87159672e-02\n",
            " -9.06966906e-03 -4.15126681e-02  4.25795233e-03 -9.35773775e-02\n",
            "  1.60550196e-02 -4.52771969e-02 -2.80875694e-02 -7.53947347e-02\n",
            "  2.66258083e-02 -7.79484445e-03 -2.31887661e-02 -5.82673140e-02\n",
            "  1.50196701e-02  5.64249866e-02 -6.65572658e-02  1.01051390e-33\n",
            " -8.96808282e-02 -1.86584331e-02 -8.24220330e-02  1.00938588e-01\n",
            " -1.57598201e-02 -5.67014627e-02 -7.54020959e-02  1.08703412e-01\n",
            " -7.39252940e-02  8.89338553e-03 -1.22846058e-02 -1.37924524e-02\n",
            "  5.99639006e-02 -3.73918476e-04  2.00049877e-02 -8.24751034e-02\n",
            "  6.70530125e-02 -3.18721235e-02  1.83520727e-02  5.03592044e-02\n",
            " -2.85759866e-02  8.23216215e-02 -6.52612075e-02 -4.28903056e-03\n",
            "  1.15322955e-02  4.90192920e-02 -4.61524464e-02  5.85129410e-02\n",
            " -2.33345442e-02  6.43910617e-02  4.09141891e-02 -1.92028731e-02\n",
            " -3.54056582e-02  1.63615309e-02 -8.51877332e-02  3.29804122e-02\n",
            "  5.05587384e-02  3.41446772e-02 -8.93918332e-03  5.12511246e-02\n",
            "  5.21134809e-02 -1.61187761e-02 -7.44320080e-02 -1.65923648e-02\n",
            " -4.47348990e-02 -1.91943347e-02 -8.38321745e-02  7.59035675e-03\n",
            "  6.05742633e-02 -4.01675291e-02 -2.44015772e-02 -7.43364990e-02\n",
            "  1.72837004e-02 -5.80018573e-02 -6.32114410e-02 -8.93812627e-02\n",
            " -4.06909920e-02 -5.72733730e-02 -7.03086480e-02 -7.30819106e-02\n",
            " -6.81691021e-02 -5.21159507e-02  7.19492277e-03 -5.63953295e-02\n",
            "  8.02311301e-03 -1.50831752e-02 -7.57755414e-02  8.94569606e-03\n",
            " -4.40710858e-02 -4.22426872e-02  8.54984000e-02  1.60540808e-02\n",
            " -6.48006275e-02  1.02356955e-01 -4.63269763e-02 -5.85574768e-02\n",
            " -6.75707757e-02 -3.36934961e-02 -9.32189170e-04 -1.17975570e-01\n",
            "  8.92525017e-02 -1.47726396e-02  2.14349497e-02  1.00321300e-01\n",
            "  4.97447290e-02  1.51924500e-02  2.66818050e-02  5.16817905e-02\n",
            " -2.57121120e-02  4.46429662e-02  6.18350040e-03  2.83740535e-02\n",
            "  3.19756418e-02  4.58125435e-02 -7.58151039e-02 -1.95212202e-08\n",
            " -5.98336384e-02 -3.25826779e-02 -1.71390120e-02  3.96752618e-02\n",
            " -1.59822050e-02 -3.55424099e-02 -6.64455369e-02  7.13788718e-02\n",
            " -1.14050079e-02 -2.38825046e-02  1.11676129e-02  2.38570962e-02\n",
            " -5.95195852e-02 -1.21477395e-02  9.31500793e-02  3.23529094e-02\n",
            "  4.05864939e-02  5.06169582e-03 -2.88126571e-03 -9.79436934e-03\n",
            "  1.00756988e-01  9.21107549e-03 -3.16128950e-03  5.85654862e-02\n",
            "  8.43484774e-02 -4.78531197e-02 -3.67804803e-02  9.15859193e-02\n",
            " -1.95948984e-02  4.68850769e-02 -4.52356711e-02  3.01217264e-03\n",
            " -1.06827259e-01  4.53749970e-02  8.57513919e-02  2.96470951e-02\n",
            " -1.11669637e-02 -1.01609841e-01  7.42863491e-02 -6.90921098e-02\n",
            "  2.31776908e-02  4.61024530e-02 -1.51653187e-02  2.79952288e-02\n",
            "  1.81482024e-02 -2.27879100e-02 -3.01896296e-02 -1.34364143e-01\n",
            "  2.91638654e-02 -2.55513266e-02 -2.37339139e-02  1.97050571e-02\n",
            "  1.51347928e-02  5.77969626e-02  1.85422855e-03 -9.11241572e-04\n",
            "  1.37972003e-02 -1.49290124e-02  8.70534964e-03  1.94904599e-02\n",
            " -2.19249027e-03  6.02292381e-02  1.43347215e-02 -4.02569734e-02]\n"
          ]
        }
      ],
      "source": [
        "index = faiss_store.index\n",
        "\n",
        "# Print total number of indexes\n",
        "print(f\"Total number of indexes: {index.ntotal}\")\n",
        "\n",
        "# Print total number of dimensions\n",
        "print(f\"Total number of dimensions: {index.d}\")\n",
        "\n",
        "# Print the embeddings for the first vector\n",
        "print(f\"First vector: {index.reconstruct(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q3LxxpcmPo2"
      },
      "source": [
        "## Retrieval System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JBpVb0a_mMi6"
      },
      "outputs": [],
      "source": [
        "# Retrieve relevant documents\n",
        "query = \"What is LLMs?\"\n",
        "k = 10\n",
        "retrieved_docs = faiss_store.similarity_search(query, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QPMcXS91mxeU"
      },
      "outputs": [],
      "source": [
        "# Build a function for retrieving documents\n",
        "def get_relevant_documents(query, k=5):\n",
        "  return faiss_store.similarity_search(query, k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnMhdQOenPT9"
      },
      "source": [
        "## Generative System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vTNH_pc6nNZ9"
      },
      "outputs": [],
      "source": [
        "# Load the LLM\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    task=\"text-generation\"\n",
        ")\n",
        "chat_model = ChatHuggingFace(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4VJCLPj5oEor"
      },
      "outputs": [],
      "source": [
        "# Define the system and human message\n",
        "def generative_system(query, context):\n",
        "  messages = [\n",
        "      SystemMessage(content = f\"\"\"\n",
        "      You are an English tour guide with a Nigerian Accent.\n",
        "      Your task is to reply in English.\n",
        "      Only answer with information from {context}.\n",
        "      If you don't know the answer, just say you don't know\n",
        "      \"\"\"),\n",
        "      HumanMessage(content=f\"Answer the {query} based on the {context}\")\n",
        "  ]\n",
        "  ai_output = chat_model.invoke(messages)\n",
        "  return display(Markdown(ai_output.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTWwQ1fKrfEz"
      },
      "source": [
        "# Combining the Retrieval and Generative System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YSxA8Wi7rWpe"
      },
      "outputs": [],
      "source": [
        "# Build the RAG system\n",
        "def rag(query):\n",
        "  context = get_relevant_documents(query)\n",
        "  return generative_system(query, context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "U9DwZxsoseNA",
        "outputId": "0bc687f5-5401-4e26-fb61-84bf4aaf8a7a"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              " LLMs, or Large Language Models, are artificial intelligence systems that are trained on vast text corpora to generate and understand natural language. They can be fine-tuned to specialize in specific domains, although Retrieval-augmented Generation (RAG) can be a cheaper and more flexible alternative. When paired with retrieval, LLMs become more reliable knowledge assistants and are increasingly used in enterprise chatbots and knowledge assistants. LLMs like GPT, LLaMA, and Mistral can all be used in RAG setups."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Test the RAG\n",
        "query = \"What are LLMs\"\n",
        "rag(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YcOx-9edssOy"
      },
      "outputs": [],
      "source": [
        "# Prepare test queries\n",
        "query_list = [\n",
        "    \"What is RAG\",\n",
        "    \"How does RAG and retrieval systems work?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "K6JQ9ruRs5XK",
        "outputId": "ecdb5d96-575c-4acf-8123-e083335735c5"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              " RAG stands for Relevance and Answers Generating systems. It is a framework for evaluating the performance of language models (LLMs) in generating answers to open-domain questions. building a RAG system typically involves three steps: embedding, indexing, and retrieval. The system embeds the queries and the text corpus, indexes the embeddings, and retrieves the most relevant text snippets based on the similarity between the query and indexed texts. RAG systems can be evaluated using metrics like precision, recall, and answer faithfulness.\n",
              "\n",
              "Evaluation of RAG systems also involves user satisfaction metrics in real-world applications. Various LLMs, such as GPT, LLaMA, and Mistral, can all be used in RAG setups. While fine-tuning allows LLMs to specialize in a domain, using RAG can be a cheaper and more flexible alternative, as it doesn't require additional annotated data or extensive computational resources."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              " RAG systems, which stand for Relevance, Adequacy, and Generality, are information retrieval systems designed to find answers to natural language queries. The process of building a RAG system generally follows three steps as stated in the document with id '4d542053-0511-4fdf-8592-17081f21d2e1': embedding, indexing, and retrieval.\n",
              "\n",
              "The embedding step involves representing text data, such as documents or queries, in a numerical format that can be processed by a machine. This often involves using techniques like word2vec, BERT, or other embedding models. This step allows the system to understand the meaning and relationships between different words and phrases in a way that can be used for efficient data processing.\n",
              "\n",
              "Next, the indexing step in RAG systems deals with creating an index of the numerical representations of the documents. This indexing process makes it possible to quickly find the most relevant documents for a given query. The indexing technique used can vary, with some RAG systems employing traditional vector space models and others using advanced techniques such as graph databases, as mentioned in the document with id '29eac862-8a01-414a-ab6d-5a0417a7122a'.\n",
              "\n",
              "Finally, during the retrieval step, RAG systems use the index produced in the previous step to identify documnets that are likely to contain the answer to the user's query. Once relevant documents are retrieved, they are passed through a language model (LLM) to generate an answer, as explained in the document with id '083470ff-8804-41b1-87ee-39ee68963360'.\n",
              "\n",
              "It is also important to note that the evaluation of a RAG system goes beyond just measuring its accuracy with metrics like precision and recall, as indicated in the document with id '654024b1-c8aa-4a87-8281-0decf8e2e921'. Answers generated by the system need to be faithful to the original document and meaningful to the user. Additionally, user satisfaction metrics can be used to evaluate performance in real-world applications as mentioned in the document with id 'ab90760a-aa6e-42f6-8e83-470793d273c3'."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Test the RAG system\n",
        "for query in query_list:\n",
        "  rag(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "f-Kou4ustJsM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {},
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
