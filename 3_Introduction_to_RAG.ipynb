{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiZ830+K5TQ7WQMUotfhEB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinakajoy/UsingLLMs-RAG-course/blob/main/3_Introduction_to_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Augumented Generation (RAG)"
      ],
      "metadata": {
        "id": "YE68DgxneTAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "lmpKNOWXecHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample documents about LLMs, RAG and Retrieval System\n",
        "documents = [\n",
        " \"Large Language Models (LLMs) are AI systems trained on massive text corpora to generate and understand natural language.\",\n",
        " \"Retrieval-Augmented Generation (RAG) improves LLMs by allowing them to fetch external knowledge when answering questions.\",\n",
        " \"Vector databases like Pinecone, Weaviate, or FAISS are commonly used to power retrieval systems.\",\n",
        " \"LLMs often struggle with outdated or missing knowledge, which is why retrieval is essential for grounding responses.\",\n",
        " \"Context windows in LLMs limit how much information can be processed at once.\",\n",
        " \"Fine-tuning allows LLMs to specialize in a domain, but RAG can be a cheaper and more flexible alternative.\",\n",
        " \"Embeddings are numeric representations of text used to measure semantic similarity in retrieval systems.\",\n",
        " \"Hybrid search combines dense vector embeddings with keyword-based search for better accuracy.\",\n",
        " \"RAG systems often follow the retrieve-then-read pipeline: fetch documents and then use an LLM to generate an answer.\",\n",
        " \"Companies use retrieval systems to provide proprietary knowledge to LLMs without retraining the base model.\",\n",
        " \"Storing context in a vector database enables systems to handle large-scale document search efficiently.\",\n",
        " \"In-house LLM deployments are often combined with retrieval systems for privacy and security.\",\n",
        " \"Retrieval systems prevent hallucinations by grounding LLM outputs in verified sources.\",\n",
        " \"Scaling retrieval pipelines requires sharding, indexing, and efficient similarity search algorithms.\",\n",
        " \"LLMs like GPT, LLaMA, and Mistral can all be used in RAG setups.\",\n",
        " \"The retrieval step typically ranks documents based on semantic closeness to the user’s query.\",\n",
        " \"Chunking documents into smaller pieces improves retrieval accuracy and relevance.\",\n",
        " \"Metadata filters in vector search allow narrowing results by tags like date, author, or topic.\",\n",
        " \"Retrieval can be combined with caching to speed up repeated queries in production systems.\",\n",
        " \"Some RAG systems use graph databases instead of vectors for knowledge-rich retrieval.\",\n",
        " \"Evaluation of RAG involves metrics like precision, recall, and answer faithfulness.\",\n",
        " \"LLMs alone are generative, but when paired with retrieval, they become more reliable knowledge assistants.\",\n",
        " \"Context storage solutions range from simple in-memory stores to distributed vector databases.\",\n",
        " \"Retrieval systems can be domain-specific, such as for healthcare, legal, or financial data.\",\n",
        " \"LLMs with RAG are increasingly used in enterprise chatbots and knowledge assistants.\",\n",
        " \"Prompt engineering is often combined with retrieval to guide LLMs in how to use the fetched data.\",\n",
        " \"Scaling foundational models often requires techniques like parameter-efficient fine-tuning or LoRA.\",\n",
        " \"Retrieval helps reduce the need for frequent fine-tuning when new knowledge becomes available.\",\n",
        " \"Latency in retrieval systems is critical, especially when powering real-time applications.\",\n",
        " \"RAG pipelines can integrate with search engines, APIs, or internal document repositories.\",\n",
        " \"Knowledge grounding ensures that LLMs provide answers supported by external evidence.\",\n",
        " \"Some retrieval systems use re-ranking models to improve the quality of the top search results.\",\n",
        " \"Building a RAG system typically involves three steps: embedding, indexing, and retrieval.\",\n",
        " \"Retrieval allows organizations to keep proprietary knowledge private while still leveraging LLMs.\",\n",
        " \"An orchestration layer manages how LLMs, retrieval, and other tools interact in complex pipelines.\",\n",
        " \"LLMs combined with RAG are often described as knowledge-enhanced generative AI.\",\n",
        " \"The retriever can use dense embeddings or sparse methods like BM25, depending on the application.\",\n",
        " \"Chunk size in embeddings has a major effect on recall and precision in retrieval.\",\n",
        " \"Open-source libraries like LangChain and LlamaIndex simplify building retrieval-augmented systems.\",\n",
        " \"Document pre-processing, such as cleaning and normalization, is critical for good retrieval performance.\",\n",
        " \"Embedding models like OpenAI’s text-embedding-ada-002 or Sentence-BERT are popular for retrieval.\",\n",
        " \"RAG pipelines can handle both structured and unstructured data sources.\",\n",
        " \"Retrieval-based systems can log citations, giving users confidence in the generated answers.\",\n",
        " \"Distributed retrieval systems use multiple servers to scale to billions of documents.\",\n",
        " \"Context windows in LLMs can be extended with retrieval, allowing access to more knowledge.\",\n",
        " \"Retrieval can be applied in multi-modal systems, not just text but also images or audio.\",\n",
        " \"Enterprises prefer retrieval over fine-tuning when their data changes frequently.\",\n",
        " \"RAG systems can be evaluated with user satisfaction metrics in real-world applications.\",\n",
        " \"Building a good retrieval system requires balancing speed, accuracy, and storage costs.\"\n",
        "]"
      ],
      "metadata": {
        "id": "QuLwfPEbeaI2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "6D0jlBLVekrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "hf_key = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "GvIviS6LehQJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = hf_key"
      ],
      "metadata": {
        "id": "2kUdgryvj58D"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-huggingface sentence-transformers langchain_community faiss-cpu -q"
      ],
      "metadata": {
        "id": "PIEF9gztj-UD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the libraries\n",
        "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain.docstore.document import Document\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "S4a5kSYekAIP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings"
      ],
      "metadata": {
        "id": "mt-a5LSBk1ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many documents we have\n",
        "print(f\"We have {len(documents)} documents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNigz6XSkz2U",
        "outputId": "2374159b-dfaa-4272-b92b-2e89bdf880ff"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 49 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap each string in a Document\n",
        "docs = [Document(page_content=text) for text in documents]"
      ],
      "metadata": {
        "id": "VBYzC_EYlAax"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a transformer-based embedding model\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "l5zEK-lblUoK"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a FAISS vector store for the docs\n",
        "faiss_store = FAISS.from_documents(docs, embedding_model)"
      ],
      "metadata": {
        "id": "xJdwx5WJlgxj"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = faiss_store.index\n",
        "\n",
        "# Print total number of indexes\n",
        "print(f\"Total number of indexes: {index.ntotal}\")\n",
        "\n",
        "# Print total number of dimensions\n",
        "print(f\"Total number of dimensions: {index.d}\")\n",
        "\n",
        "# Print the embeddings for the first vector\n",
        "print(f\"First vector: {index.reconstruct(0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9cB3_qxlx0T",
        "outputId": "139a15d0-4c7b-43d1-8f86-c691eaebb24f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of indexes: 49\n",
            "Total number of dimensions: 384\n",
            "First vector: [ 5.59799224e-02 -1.23615697e-01  5.04221395e-02 -2.13914042e-04\n",
            "  7.37531185e-02  8.95548984e-03 -6.00405224e-02  1.49373375e-02\n",
            "  5.68501055e-02  3.37935537e-02 -2.43983790e-02  7.96056166e-03\n",
            "  6.15357235e-02 -8.11576284e-03  1.36438720e-02  5.61926477e-02\n",
            "  7.67754018e-02  3.06968205e-03 -2.04155762e-02 -9.15290713e-02\n",
            "  7.54889697e-02  9.49257165e-02 -2.44868174e-02  7.48942839e-03\n",
            "  2.30758637e-02  3.10892090e-02 -7.53808301e-03 -1.41408620e-02\n",
            "  4.08316962e-02 -4.55593131e-02  1.01234503e-02  5.85593507e-02\n",
            "  7.60235637e-02  8.76820683e-02 -2.13428158e-02  6.30844012e-02\n",
            " -6.90986440e-02  4.78140311e-03  2.68640146e-02  6.28683064e-03\n",
            " -2.52215122e-03 -6.12178817e-03  7.45765865e-02 -6.18263520e-02\n",
            "  1.61846399e-01  4.47338596e-02 -8.18227157e-02  1.99009180e-02\n",
            " -4.68176790e-02  2.07310989e-02 -1.00719728e-01 -5.11116572e-02\n",
            " -1.06674517e-02  3.76786180e-02 -3.49273942e-02 -7.82647058e-02\n",
            "  3.20381373e-02 -1.67206535e-03 -3.04974969e-02 -4.60983142e-02\n",
            " -2.90375128e-02 -1.11909561e-01 -3.08425631e-02  2.96229981e-02\n",
            "  4.88636158e-02  2.27161609e-02 -1.27594126e-03  9.00783837e-02\n",
            " -2.33559106e-02 -5.88586181e-02  2.04303097e-02 -1.91230632e-04\n",
            " -2.00297628e-02  9.18395296e-02 -2.62243543e-02 -1.59554258e-02\n",
            "  2.46915072e-02  5.87892253e-03  1.11701906e-01 -2.40288768e-02\n",
            " -2.62684207e-02  6.26533329e-02  8.76337662e-02  1.01037351e-02\n",
            "  2.51335502e-02  2.29502912e-03  8.02783854e-03  4.09501828e-02\n",
            "  3.01650800e-02  1.23733953e-02 -9.87364165e-03 -1.33968309e-01\n",
            " -1.48515413e-02  2.84969490e-02 -1.95497740e-02  3.95998135e-02\n",
            " -2.23897658e-02 -4.30137143e-02  5.59214354e-02  4.12259735e-02\n",
            "  2.94184778e-02  7.40719140e-02  5.86774610e-02 -4.84780110e-02\n",
            " -6.83995113e-02 -2.56933831e-02  4.58729863e-02  2.31131911e-02\n",
            "  7.23061934e-02 -8.96524489e-02  8.51430057e-04  4.17844579e-02\n",
            "  1.86159015e-02  3.15907858e-02  7.67726600e-02 -1.13299012e-01\n",
            "  9.04402807e-02 -3.27261426e-02  1.91022847e-02  4.26131785e-02\n",
            " -9.80662405e-02  2.36903522e-02  1.28346484e-03  1.46677978e-02\n",
            "  1.14158010e-02  2.93858796e-02 -5.92334569e-02 -1.35153455e-33\n",
            "  6.24970458e-02  4.11282554e-02  9.54749435e-03  7.97725841e-02\n",
            "  6.24811426e-02 -1.60833308e-03  6.43271282e-02  7.05759376e-02\n",
            " -4.89395298e-02  7.55818840e-03 -8.24661851e-02  1.46082575e-02\n",
            " -6.78726565e-03  2.84265727e-02  3.04525718e-02 -2.73350812e-02\n",
            "  3.22145788e-04 -2.51294905e-03  5.46441227e-03 -4.16599847e-02\n",
            "  1.62200034e-02  2.48758085e-02  5.12611046e-02 -6.98485672e-02\n",
            "  2.62655709e-02  4.06459160e-02  8.73463750e-02 -5.45662232e-02\n",
            "  2.54323445e-02  1.68162603e-02 -7.06314445e-02  1.07546449e-02\n",
            " -1.46114072e-02  1.00386357e-02  2.65516695e-02  1.60414248e-03\n",
            " -7.74145573e-02 -8.63365177e-03  4.12104949e-02 -6.52849525e-02\n",
            "  4.83007589e-03  3.51354145e-02  3.35920639e-02 -3.78179103e-02\n",
            " -7.55131394e-02  4.55202870e-02 -1.80512276e-02 -4.48966287e-02\n",
            "  1.89132628e-03 -5.54836839e-02  8.75203237e-02 -9.03356820e-03\n",
            " -7.12265894e-02 -1.28765805e-02  5.74364401e-02  6.26431331e-02\n",
            " -5.76011203e-02  1.13319550e-02 -4.01937962e-02  1.42380744e-02\n",
            "  2.04041079e-02  2.93308180e-02  3.38891596e-02  7.32148066e-02\n",
            "  1.33543387e-01  2.68958621e-02 -5.48977405e-02  3.05433571e-02\n",
            "  8.66482034e-02 -3.48206162e-02  2.96646748e-02 -3.14270183e-02\n",
            " -1.71329379e-02 -1.05607854e-02 -3.67758982e-02 -7.99657255e-02\n",
            "  9.06064268e-03 -6.87769204e-02  1.05948746e-02  6.87159672e-02\n",
            " -9.06966906e-03 -4.15126681e-02  4.25795233e-03 -9.35773775e-02\n",
            "  1.60550196e-02 -4.52771969e-02 -2.80875694e-02 -7.53947347e-02\n",
            "  2.66258083e-02 -7.79484445e-03 -2.31887661e-02 -5.82673140e-02\n",
            "  1.50196701e-02  5.64249866e-02 -6.65572658e-02  1.01051390e-33\n",
            " -8.96808282e-02 -1.86584331e-02 -8.24220330e-02  1.00938588e-01\n",
            " -1.57598201e-02 -5.67014627e-02 -7.54020959e-02  1.08703412e-01\n",
            " -7.39252940e-02  8.89338553e-03 -1.22846058e-02 -1.37924524e-02\n",
            "  5.99639006e-02 -3.73918476e-04  2.00049877e-02 -8.24751034e-02\n",
            "  6.70530125e-02 -3.18721235e-02  1.83520727e-02  5.03592044e-02\n",
            " -2.85759866e-02  8.23216215e-02 -6.52612075e-02 -4.28903056e-03\n",
            "  1.15322955e-02  4.90192920e-02 -4.61524464e-02  5.85129410e-02\n",
            " -2.33345442e-02  6.43910617e-02  4.09141891e-02 -1.92028731e-02\n",
            " -3.54056582e-02  1.63615309e-02 -8.51877332e-02  3.29804122e-02\n",
            "  5.05587384e-02  3.41446772e-02 -8.93918332e-03  5.12511246e-02\n",
            "  5.21134809e-02 -1.61187761e-02 -7.44320080e-02 -1.65923648e-02\n",
            " -4.47348990e-02 -1.91943347e-02 -8.38321745e-02  7.59035675e-03\n",
            "  6.05742633e-02 -4.01675291e-02 -2.44015772e-02 -7.43364990e-02\n",
            "  1.72837004e-02 -5.80018573e-02 -6.32114410e-02 -8.93812627e-02\n",
            " -4.06909920e-02 -5.72733730e-02 -7.03086480e-02 -7.30819106e-02\n",
            " -6.81691021e-02 -5.21159507e-02  7.19492277e-03 -5.63953295e-02\n",
            "  8.02311301e-03 -1.50831752e-02 -7.57755414e-02  8.94569606e-03\n",
            " -4.40710858e-02 -4.22426872e-02  8.54984000e-02  1.60540808e-02\n",
            " -6.48006275e-02  1.02356955e-01 -4.63269763e-02 -5.85574768e-02\n",
            " -6.75707757e-02 -3.36934961e-02 -9.32189170e-04 -1.17975570e-01\n",
            "  8.92525017e-02 -1.47726396e-02  2.14349497e-02  1.00321300e-01\n",
            "  4.97447290e-02  1.51924500e-02  2.66818050e-02  5.16817905e-02\n",
            " -2.57121120e-02  4.46429662e-02  6.18350040e-03  2.83740535e-02\n",
            "  3.19756418e-02  4.58125435e-02 -7.58151039e-02 -1.95212202e-08\n",
            " -5.98336384e-02 -3.25826779e-02 -1.71390120e-02  3.96752618e-02\n",
            " -1.59822050e-02 -3.55424099e-02 -6.64455369e-02  7.13788718e-02\n",
            " -1.14050079e-02 -2.38825046e-02  1.11676129e-02  2.38570962e-02\n",
            " -5.95195852e-02 -1.21477395e-02  9.31500793e-02  3.23529094e-02\n",
            "  4.05864939e-02  5.06169582e-03 -2.88126571e-03 -9.79436934e-03\n",
            "  1.00756988e-01  9.21107549e-03 -3.16128950e-03  5.85654862e-02\n",
            "  8.43484774e-02 -4.78531197e-02 -3.67804803e-02  9.15859193e-02\n",
            " -1.95948984e-02  4.68850769e-02 -4.52356711e-02  3.01217264e-03\n",
            " -1.06827259e-01  4.53749970e-02  8.57513919e-02  2.96470951e-02\n",
            " -1.11669637e-02 -1.01609841e-01  7.42863491e-02 -6.90921098e-02\n",
            "  2.31776908e-02  4.61024530e-02 -1.51653187e-02  2.79952288e-02\n",
            "  1.81482024e-02 -2.27879100e-02 -3.01896296e-02 -1.34364143e-01\n",
            "  2.91638654e-02 -2.55513266e-02 -2.37339139e-02  1.97050571e-02\n",
            "  1.51347928e-02  5.77969626e-02  1.85422855e-03 -9.11241572e-04\n",
            "  1.37972003e-02 -1.49290124e-02  8.70534964e-03  1.94904599e-02\n",
            " -2.19249027e-03  6.02292381e-02  1.43347215e-02 -4.02569734e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval System"
      ],
      "metadata": {
        "id": "0Q3LxxpcmPo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve relevant documents\n",
        "query = \"What is LLMs?\"\n",
        "k = 10\n",
        "retrieved_docs = faiss_store.similarity_search(query, k)"
      ],
      "metadata": {
        "id": "JBpVb0a_mMi6"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a function for retrieving documents\n",
        "def get_relevant_documents(query, k=5):\n",
        "  return faiss_store.similarity_search(query, k)"
      ],
      "metadata": {
        "id": "QPMcXS91mxeU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generative System"
      ],
      "metadata": {
        "id": "dnMhdQOenPT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the LLM\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"microsoft/Phi-4\",\n",
        "    task=\"text-generation\"\n",
        ")\n",
        "chat_model = ChatHuggingFace(llm=llm)"
      ],
      "metadata": {
        "id": "vTNH_pc6nNZ9"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the system and human message\n",
        "def generative_system(query, context):\n",
        "  messages = [\n",
        "      SystemMessage(content = f\"\"\"\n",
        "      You are an English tour guide with a Nigerian Accent.\n",
        "      Your task is to reply in English.\n",
        "      Only answer with information from {context}.\n",
        "      If you don't know the answer, just say you don't know\n",
        "      \"\"\"),\n",
        "      HumanMessage(content=f\"Answer the {query} based on the {context}\")\n",
        "  ]\n",
        "  ai_output = chat_model.invoke(messages)\n",
        "  return display(Markdown(ai_output.content))"
      ],
      "metadata": {
        "id": "4VJCLPj5oEor"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining the Retrieval and Generative System"
      ],
      "metadata": {
        "id": "FTWwQ1fKrfEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the RAG system\n",
        "def rag(query):\n",
        "  context = get_relevant_documents(query)\n",
        "  return generative_system(query, context)"
      ],
      "metadata": {
        "id": "YSxA8Wi7rWpe"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the RAG\n",
        "query = \"What are LLMs\"\n",
        "rag(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "U9DwZxsoseNA",
        "outputId": "de09cfbe-77b3-457b-8b32-c19fd2366681"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Ah, my friend, Large Language Models, or LLMs as we call them, are essentially AI systems that are trained on massive amounts of text. Their purpose is to generate and understand natural language. These models, some of which you might know include GPT, LLaMA, and Mistral, are designed to be generative.\n\nBut here's where it gets interesting—when LLMs are paired with something called retrieval, they become more than just text generators. They transform into more reliable knowledge assistants. This combination, known as Retrieval-Augmented Generation or RAG, significantly boosts their ability to provide accurate information. You see, LLMs can give answers with an intelligent flair, but with RAG, they bring in the reliability factor, fetching reliable knowledge on the fly. Quite useful in enterprise settings, wouldn't you say? They're used in chatbots, knowledge assistants, and a variety of applications across industries.\n\nSo, while on their own these models are powerful, the integration with retrieval makes them more dependable, broadening the horizon of what they can achieve!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare test queries\n",
        "query_list = [\n",
        "    \"What is RAG\",\n",
        "    \"How does RAG and retrieval systems work?\"\n",
        "]"
      ],
      "metadata": {
        "id": "YcOx-9edssOy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the RAG system\n",
        "for query in query_list:\n",
        "  rag(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "K6JQ9ruRs5XK",
        "outputId": "66ce1bd5-4f92-4924-fcd6-57e0d308fe59"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "RAG, which stands for Retrieval Augmented Generation, is a system designed to enhance language models by integrating document retrieval with language generation capabilities. Building a RAG system typically involves three steps: embedding, indexing, and retrieval. These steps are crucial for enabling the RAG to access and use information from large sets of documents effectively. \n\nIn RAG setups, LLMs (Large Language Models) like GPT, LLaMA, and Mistral are commonly used to leverage the combination of retrieval and generation for tasks. Unlike fine-tuning, which allows LLMs to specialize in a specific domain, RAG offers a more flexible and cost-effective alternative by dynamically retrieving relevant information from external sources during inference.\n\nRAG systems can be evaluated using metrics like precision, recall, answer faithfulness, and user satisfaction in real-world applications to assess their performance and effectiveness. This evaluation emphasizes the system's ability to provide accurate and contextually appropriate responses."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Ah, delightful question! A RAG system, or Retrieval-Augmented Generation system, operates primarily through a process known as the retrieve-then-read pipeline. To begin, it involves three crucial steps: embedding, indexing, and retrieval. Here's a breezy explanation:\n\n1. **Embedding**: This is where documents or data are converted into numerical vectors that a computer can process. It's sort of like translating items into a universal language computers understand well!\n\n2. **Indexing**: After embedding, the vectors are organized and stored in an efficient way for quick access. Think of it as an indexing system like you’ll find in a library, to make findin' document data a breeze.\n\n3. **Retrieval**: In this stage, the system fetches the most relevant documents based on the input query. This is like walking into a shop and finding exactly what you're searching for amid a crowd of items.\n\nMany systems follow a \"retrieve-then-read\" approach. They first fetch relevant documents based on a query and then use a large language model, or LLM, to generate an answer that takes the retrieved documents into account.\n\nInterestingly, some RAG systems even use graph databases for a more knowledge-rich retrieval as opposed to traditional vector approaches. This can offer tremendous benefits in handling complex queries! Evaluation of these systems involves looking at metrics such as precision, recall, and answer faithfulness, and in real-world applications, user satisfaction metrics are also considered.\n\nThat sums it up! If you have more queries, feel free to ask, mon ami!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f-Kou4ustJsM"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}